\section{Evaluation}
\subsection{YOLO Object Detection}
Our main goal is to never miss the detection of a potential license plate. This
means that we favor recall over precision, which means that we want the correct
license plate amongst the set of detected objects even if this means an
increased number of false positives.

Our solution achieves \todo{}$\%$ precision on our test data. test

\begin{figure}
    \begin{subfigure}[b]{.55\textwidth}
        \includegraphics[width=\textwidth]{figures/yolo/1000.jpg}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{.15\textwidth}
        \includegraphics[width=\textwidth]{figures/yolo/1000_montage.png}
    \end{subfigure}
    \hfill\hfill
    \\
    \begin{subfigure}[b]{.55\textwidth}
        \includegraphics[width=\textwidth]{figures/yolo/179.jpg}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{.4\textwidth}
        \includegraphics[width=\textwidth]{figures/yolo/179_montage.png}
    \end{subfigure}
    \hfill
    \label{fig:yolo-detection-montages}
    \caption{Example YOLO detections, and the corresponding cut outs.
        Background elements might get incorrectly labeled as number plates.  As
        we mention later on, our goal is to get a high as possible recall. If
        a lower false positive count was desired, we could increase the
    confidence threshold.}
\end{figure}

Figure~\ref{fig:yolo-detection-montages}  shows random detections in different
scenarios. We cut out each of the detected objects to separate image files. Some
examples can be seen on Figure~\ref{fig:cutout-montage}.
These cut out detections serve as the input of the next step of our \ac{ALPR}
pipeline.
\begin{figure}
    \includegraphics[width=\textwidth]{figures/yolo/cutout_montage.jpg}
    \label{fig:cutout-montage}
    \caption{Some example object detection results that serve as the
    input for the OCR stage.}
\end{figure}

\subsection{Optical Character Recognition (OCR)}

\subsection{Qualitative Comparison}
For qualitative comparison, we consider scenarios with viewing conditions
corresponding to more difficult visual settings. We categorize these cases by
perceived difficulty, and show examples. Figure \todo{add figure} shows
different settings. \todo[inline]{Evaluate which one is the hardest setting.}

These are some of the settings we expect to make \ac{ALPR} more
difficult:\todo[inline]{Create these comparisons, plot them in a figure.}
\begin{itemize}
    \item Direct sunlight 
    \item Partial occlusion of the license plate 
    \item Miscellaneous weather effects such as rain or fog
    \item Small size of the licence plate (i.e. the object is further away)
\end{itemize}

\subsection{Final Test}
As with any learning-based method, there is an inherent uncertainty on the
generalization capabilities of our solution. Although we separeted our data into
training, validation, and test data, these datasets might still exhibit
similarities, and sampling test data out-of-distribution is inherently
difficult.

Thus, as a final test, we close the evaluation of our project with evaluating
our solution \textit{in the wild}, measuring its robustness on real-life data.

\todo[inline]{privacy $\to$ blur the actual car?}
\todo[inline]{Add images taken with our phone camera.}
