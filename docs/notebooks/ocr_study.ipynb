{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "history_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Optical character recognition**"
      ],
      "metadata": {
        "id": "b_ASk2EUTQe1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Previous work**\n",
        "\n",
        "Survey of some of the previously developed OCR software products and technologies.\n",
        "\n",
        "\n",
        "##**CuneiForm**\n",
        "\n",
        "by Russian software company Cognitive Technologies\n",
        "**Open-source**\n",
        "[Link](https://www.linuxlinks.com/cuneiform/)\n",
        "\n",
        "\n",
        "##**Tesseract**\n",
        "\n",
        "Originally developed by Hewlett-Packard as proprietary software in the 1980s\n",
        "Supervised by Google.\n",
        "**Apache License**\n",
        "[Link](https://github.com/tesseract-ocr)\n",
        "\n",
        "\n",
        "##**GdPicture OCR SDK**\n",
        "\n",
        "GdPicture includes an Optical Character Recognition engine to develop any kind of application requiring OCR technology.\n",
        "**Not open-source!**\n",
        "[Link](https://www.gdpicture.com/ocr-sdk/)\n",
        "\n",
        "\n",
        "##**Transym**\n",
        "\n",
        "TOCR is a one of the most sophisticated OCR software packages on the market, specifically designed for ease of integration. With a free API and attractive volume pricing, it is designed for system integrators.\n",
        "**Not open-source!**\n",
        "[Link](https://transym.com/)\n",
        "\n",
        "\n",
        "##**Computer Vision**\n",
        "\n",
        "by Microsoft\n",
        "An AI service that analyzes content in images and video\n",
        "\n",
        "Boost content discoverability, automate text extraction, analyze video in real time, and create products that more people can use by embedding cloud vision capabilities in your apps with Computer Vision, part of Azure Cognitive Services. Use visual data processing to label content with objects and concepts, extract text, generate image descriptions, moderate content, and understand people's movement in physical spaces. No machine learning expertise is required.\n",
        "\n",
        "**Azure service**\n",
        "[Link](https://azure.microsoft.com/en-us/products/cognitive-services/computer-vision/#overview)\n",
        "\n",
        "\n",
        "##**Google AI**\n",
        "\n",
        "[Link](https://ai.google/tools/#developers)\n",
        "\n",
        "\n",
        "##**Amazon Textract**\n",
        "\n",
        "\"Automatically extract printed text, handwriting, and data from any document\"\n",
        "\n",
        "**AWS component**\n",
        "[Link](https://aws.amazon.com/textract/)\n",
        "\n",
        "\n",
        "##**Kadmos**\n",
        "\n",
        "from reRecognition\n",
        "\n",
        "KADMOS is an ICR and OCR technology for developers built on unique “discriminatory entropy”methodology that does not depend on dictionaries. KADMOS offers exceptionally fast processing speeds in over 200 languages for both hand-written and machine printed text.\n",
        "\n",
        "Provided as a feature rich SDK, KADMOS can be integrated into the most sophisticated applications where mission critical levels of accuracy and speed are required.\n",
        "\n",
        "KADMOS is used extensively in document management, finance, healthcare, education, government and defence applications around the world and selected by developers for its unique capabilities.\n",
        "\n",
        "**Not open-source!**\n",
        "[Link](https://rerecognition.com/products/)\n",
        "\n",
        "\n",
        "##**Mindee**\n",
        "\n",
        "Python OCR SDK\n",
        "Supports invoice, passport, receipt OCR APIs and custom-built API from the API Builder.\n",
        "\n",
        "**MIT license**\n",
        "\n",
        "[Getting started](https://developers.mindee.com/docs/python-getting-started)\n",
        "\n",
        "[Source code](https://github.com/mindee/mindee-api-python)\n",
        "\n",
        "##**ocr.pytorch**\n",
        "An open-source project on GitHub\n",
        "Pure pytorch implemented OCR project including text detection and recognition.\n",
        "\n",
        "**MIT license**\n",
        "\n",
        "[Repository](https://github.com/courao/ocr.pytorch)\n",
        "\n",
        "##**TextRecognitionDataGenerator**\n",
        "\n",
        "A synthetic data generator for text recognition\n",
        "\n",
        "**MIT license**\n",
        "\n",
        "[Link](https://github.com/Belval/TextRecognitionDataGenerator)\n",
        "\n",
        "\n",
        "##**License plate font**\n",
        "\n",
        ".ttf font\n",
        "\n",
        "open for commercial use\n",
        "\n",
        "\n",
        "[Link](https://www.1001fonts.com/license-plates-fonts.html)\n",
        "\n",
        "\n",
        "##**B. Shi, X. Bai and C. Yao**\n",
        "\n",
        "B. Shi, X. Bai and C. Yao, \"An End-to-End Trainable Neural Network for Image-Based Sequence Recognition and Its Application to Scene Text Recognition,\" in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 39, no. 11, pp. 2298-2304, 1 Nov. 2017, doi: 10.1109/TPAMI.2016.2646371.\n",
        "\n",
        "[Link](https://ieeexplore.ieee.org/abstract/document/7801919?casa_token=l7alCR-iNckAAAAA:XjdU8cR5KEKqRJKS48UZSxoyXqM5LGl4llw-HUIh9UbSAiK9v3zPyD9gYvwjsngkrRW-HEh1_Q)\n",
        "\n",
        "\n",
        "##**Synthetic Data and Artificial Neural Networks for Natural Scene Text Recognition**\n",
        "\n",
        "Max Jaderberg, Karen Simonyan, Andrea Vedaldi, Andrew Zisserman\n",
        "\n",
        "[Link](https://arxiv.org/abs/1406.2227)\n",
        "\n",
        "\n",
        "##**Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks**\n",
        "\n",
        "Alex Graves, Santiago Fernández, Faustino Gomez, and Jürgen Schmidhuber. 2006. Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks. In Proceedings of the 23rd international conference on Machine learning (ICML '06). Association for Computing Machinery, New York, NY, USA, 369–376. https://doi.org/10.1145/1143844.1143891\n",
        "\n",
        "[Link](https://dl.acm.org/doi/abs/10.1145/1143844.1143891?casa_token=e19selHVFKsAAAAA:mrb7JOGyGF84MfHGUcpvhE1AzuC-_fUKIYI-yHsZa8wpuyR0ld5xzCTmUNT0hU3xgi6aJyXkA9Zn)\n",
        "\n"
      ],
      "metadata": {
        "id": "Ad93UD1tcmnC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Learning materials for building Pytorch OCR application\n",
        "\n",
        "- [Building a custom OCR using pytorch](https://deepayan137.github.io/blog/markdown/2020/08/29/building-ocr.html)\n",
        "-[I need advice for a OCR project](https://discuss.pytorch.org/t/i-need-advice-for-a-ocr-project/113087)\n",
        "-[Simon Z. Python kurzus](https://theflyingpiano99.github.io/PythonBeginnerCourse/index.html)\n",
        "-[Pytorch official tutorials](https://pytorch.org/tutorials/)\n",
        "\n"
      ],
      "metadata": {
        "id": "2640Kj5ClFp_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#OCR study\n",
        "\n",
        "In this section we make notes about the architecture of the OCR system under design.\n",
        "The ideas in this section originate mainly from [Building a custom OCR using pytorch](https://deepayan137.github.io/blog/markdown/2020/08/29/building-ocr.html#setting-up-the-data).\n",
        "\n",
        "\n",
        "##Setting up the Data\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3LLYTENflsSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title installing sample data generator\n",
        "!pip install trdg\n",
        "!pip3 install pillow==6.2.2 \n",
        "!pip3 install torch_utils\n",
        "!pip3 install torchvision\n",
        "!pip3 install pytorch_wrapper\n",
        "!pip3 install textdistance\n",
        "!cp -r ./drive/MyDrive/Adapting-OCR-master/* ./\n",
        "#!pip3 install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6lPsMnKgrPHc",
        "outputId": "60530db3-69b3-40d6-955e-9d8d354c9d4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: trdg in /usr/local/lib/python3.7/dist-packages (1.8.0)\n",
            "Requirement already satisfied: diffimg==0.2.3 in /usr/local/lib/python3.7/dist-packages (from trdg) (0.2.3)\n",
            "Requirement already satisfied: tqdm>=4.23.0 in /usr/local/lib/python3.7/dist-packages (from trdg) (4.64.1)\n",
            "Requirement already satisfied: wikipedia>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from trdg) (1.4.0)\n",
            "Requirement already satisfied: python-bidi==0.4.2 in /usr/local/lib/python3.7/dist-packages (from trdg) (0.4.2)\n",
            "Requirement already satisfied: arabic-reshaper==2.1.3 in /usr/local/lib/python3.7/dist-packages (from trdg) (2.1.3)\n",
            "Collecting pillow>=7.0.0\n",
            "  Using cached Pillow-9.3.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "Requirement already satisfied: opencv-python>=4.2.0.32 in /usr/local/lib/python3.7/dist-packages (from trdg) (4.6.0.66)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from trdg) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from arabic-reshaper==2.1.3->trdg) (57.4.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from arabic-reshaper==2.1.3->trdg) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from python-bidi==0.4.2->trdg) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python>=4.2.0.32->trdg) (1.21.6)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->trdg) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->trdg) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->trdg) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->trdg) (1.24.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from wikipedia>=1.4.0->trdg) (4.6.3)\n",
            "Installing collected packages: pillow\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 6.2.2\n",
            "    Uninstalling Pillow-6.2.2:\n",
            "      Successfully uninstalled Pillow-6.2.2\n",
            "Successfully installed pillow-9.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pillow==6.2.2\n",
            "  Using cached Pillow-6.2.2-cp37-cp37m-manylinux1_x86_64.whl (2.1 MB)\n",
            "Installing collected packages: pillow\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 9.3.0\n",
            "    Uninstalling Pillow-9.3.0:\n",
            "      Successfully uninstalled Pillow-9.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "trdg 1.8.0 requires pillow>=7.0.0, but you have pillow 6.2.2 which is incompatible.\n",
            "bokeh 2.3.3 requires pillow>=7.1.0, but you have pillow 6.2.2 which is incompatible.\u001b[0m\n",
            "Successfully installed pillow-6.2.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch_utils in /usr/local/lib/python3.7/dist-packages (0.1.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torch_utils) (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torch_utils) (4.1.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.13.1+cu113)\n",
            "Requirement already satisfied: torch==1.12.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.12.1+cu113)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchvision) (4.1.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (6.2.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/packaging/version.py\", line 57, in parse\n",
            "    return Version(version)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/packaging/version.py\", line 298, in __init__\n",
            "    raise InvalidVersion(\"Invalid version: '{0}'\".format(version))\n",
            "pip._vendor.packaging.version.InvalidVersion: Invalid version: 'aeppl'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/autocompletion.py\", line 10, in <module>\n",
            "    from pip._internal.cli.main_parser import create_main_parser\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/main_parser.py\", line 8, in <module>\n",
            "    from pip._internal.cli import cmdoptions\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/cmdoptions.py\", line 23, in <module>\n",
            "    from pip._internal.cli.parser import ConfigOptionParser\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/parser.py\", line 12, in <module>\n",
            "    from pip._internal.configuration import Configuration, ConfigurationError\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/configuration.py\", line 21, in <module>\n",
            "    from pip._internal.exceptions import (\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/exceptions.py\", line 7, in <module>\n",
            "    from pip._vendor.pkg_resources import Distribution\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3251, in <module>\n",
            "    @_call_aside\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3235, in _call_aside\n",
            "    f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3287, in _initialize_master_working_set\n",
            "    list(map(working_set.add_entry, sys.path))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 623, in add_entry\n",
            "    for dist in find_distributions(entry, True):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2061, in find_on_path\n",
            "    path_item_entries = _by_version_descending(filtered)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2034, in _by_version_descending\n",
            "    return sorted(names, key=_by_version, reverse=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2032, in _by_version\n",
            "    return [packaging.version.parse(part) for part in parts]\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2032, in <listcomp>\n",
            "    return [packaging.version.parse(part) for part in parts]\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/packaging/version.py\", line 59, in parse\n",
            "    return LegacyVersion(version)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/packaging/version.py\", line 125, in __init__\n",
            "    self._key = _legacy_cmpkey(self._version)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/packaging/version.py\", line 238, in _legacy_cmpkey\n",
            "    for part in _parse_version_parts(version.lower()):\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 5, in <module>\n",
            "    from pip._internal.cli.main import main\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/main.py\", line 9, in <module>\n",
            "    from pip._internal.cli.autocompletion import autocomplete\n",
            "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 320, in __exit__\n",
            "KeyboardInterrupt\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: textdistance in /usr/local/lib/python3.7/dist-packages (4.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Generating sample data\n",
        "!trdg -i words.txt -c 20 --output_dir data/train -ft fonts/din1451alt.ttf\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQFK1dJeqzjm",
        "outputId": "173c3e92-48f4-443d-f3e3-bc311925009b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cannot open font\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ueLbWEY1S4_6"
      },
      "outputs": [],
      "source": [
        "#@title Importing libraries\n",
        "\n",
        "# Source code from https://deepayan137.github.io/blog/markdown/2020/08/29/building-ocr.html#setting-up-the-data\n",
        "\n",
        "# Install missing dependencies:\n",
        "\n",
        "\n",
        "#-------------------------------------------------------\n",
        "\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import pdb\n",
        "import six\n",
        "import random\n",
        "import lmdb\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import math\n",
        "from collections import OrderedDict\n",
        "from itertools import chain\n",
        "import logging\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import sampler\n",
        "import torchvision.transforms as transforms\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR\n",
        "from torch.nn.utils.clip_grad import clip_grad_norm_\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "sys.path.insert(0, '../')\n",
        "from src.utils.utils import AverageMeter, Eval, OCRLabelConverter\n",
        "from src.utils.utils import EarlyStopping, gmkdir\n",
        "from src.optim.optimizer import STLR\n",
        "from src.utils.utils import gaussian\n",
        "from tqdm import *\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Dataset\n",
        "\n",
        "# Source code from https://deepayan137.github.io/blog/markdown/2020/08/29/building-ocr.html#setting-up-the-data\n",
        "\n",
        "class SynthDataset(Dataset):\n",
        "    def __init__(self, opt):\n",
        "        super(SynthDataset, self).__init__()\n",
        "        self.path = os.path.join(opt['path'], opt['imgdir'])\n",
        "        self.images = os.listdir(self.path)\n",
        "        self.nSamples = len(self.images)\n",
        "        f = lambda x: os.path.join(self.path, x)\n",
        "        self.imagepaths = list(map(f, self.images))\n",
        "       \ttransform_list =  [transforms.Grayscale(1),\n",
        "                            transforms.ToTensor(), \n",
        "                            transforms.Normalize((0.5,), (0.5,))]\n",
        "        self.transform = transforms.Compose(transform_list)\n",
        "        self.collate_fn = SynthCollator()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.nSamples\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        assert index <= len(self), 'index range error'\n",
        "        imagepath = self.imagepaths[index]\n",
        "        imagefile = os.path.basename(imagepath)\n",
        "        img = Image.open(imagepath)\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "        item = {'img': img, 'idx':index}\n",
        "        item['label'] = imagefile.split('_')[0]\n",
        "        return item "
      ],
      "metadata": {
        "id": "HcAFfCfqcnax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Synth collator\n",
        "\n",
        "# Source code from https://deepayan137.github.io/blog/markdown/2020/08/29/building-ocr.html#setting-up-the-data\n",
        "\n",
        "class SynthCollator(object):\n",
        "    \n",
        "    def __call__(self, batch):\n",
        "        width = [item['img'].shape[2] for item in batch]\n",
        "        indexes = [item['idx'] for item in batch]\n",
        "        imgs = torch.ones([len(batch), batch[0]['img'].shape[0], batch[0]['img'].shape[1], \n",
        "                           max(width)], dtype=torch.float32)\n",
        "        for idx, item in enumerate(batch):\n",
        "            try:\n",
        "                imgs[idx, :, :, 0:item['img'].shape[2]] = item['img']\n",
        "            except:\n",
        "                print(imgs.shape)\n",
        "        item = {'img': imgs, 'idx':indexes}\n",
        "        if 'label' in batch[0].keys():\n",
        "            labels = [item['label'] for item in batch]\n",
        "            item['label'] = labels\n",
        "        return item"
      ],
      "metadata": {
        "id": "ioH9WdBvc5ET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Defining our Model\n",
        "\n",
        "Architecture proposed by [B. Shi et al.].\n",
        "\n",
        "![image.png](https://deepayan137.github.io/blog/images/crnn.png)"
      ],
      "metadata": {
        "id": "l5pOYe9ksePV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Convolutional RNN layer & bidir. LSTM layer\n",
        "\n",
        "# Source code from https://deepayan137.github.io/blog/markdown/2020/08/29/building-ocr.html#setting-up-the-data\n",
        "\n",
        "class BidirectionalLSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, nIn, nHidden, nOut):\n",
        "        super(BidirectionalLSTM, self).__init__()\n",
        "        self.rnn = nn.LSTM(nIn, nHidden, bidirectional=True)\n",
        "        self.embedding = nn.Linear(nHidden * 2, nOut)\n",
        "    def forward(self, input):\n",
        "        self.rnn.flatten_parameters()\n",
        "        recurrent, _ = self.rnn(input)\n",
        "        T, b, h = recurrent.size()\n",
        "        t_rec = recurrent.view(T * b, h)\n",
        "        output = self.embedding(t_rec)  # [T * b, nOut]\n",
        "        output = output.view(T, b, -1)\n",
        "        return output\n",
        "\n",
        "\n",
        "class CRNN(nn.Module):\n",
        "\n",
        "    def __init__(self, opt, leakyRelu=False):\n",
        "        super(CRNN, self).__init__()\n",
        "\n",
        "        assert opt['imgH'] % 16 == 0, 'imgH has to be a multiple of 16'\n",
        "\n",
        "        ks = [3, 3, 3, 3, 3, 3, 2]\n",
        "        ps = [1, 1, 1, 1, 1, 1, 0]\n",
        "        ss = [1, 1, 1, 1, 1, 1, 1]\n",
        "        nm = [64, 128, 256, 256, 512, 512, 512]\n",
        "\n",
        "        cnn = nn.Sequential()\n",
        "\n",
        "        def convRelu(i, batchNormalization=False):\n",
        "            nIn = opt['nChannels'] if i == 0 else nm[i - 1]\n",
        "            nOut = nm[i]\n",
        "            cnn.add_module('conv{0}'.format(i),\n",
        "                           nn.Conv2d(nIn, nOut, ks[i], ss[i], ps[i]))\n",
        "            if batchNormalization:\n",
        "                cnn.add_module('batchnorm{0}'.format(i), nn.BatchNorm2d(nOut))\n",
        "            if leakyRelu:\n",
        "                cnn.add_module('relu{0}'.format(i),\n",
        "                               nn.LeakyReLU(0.2, inplace=True))\n",
        "            else:\n",
        "                cnn.add_module('relu{0}'.format(i), nn.ReLU(True))\n",
        "\n",
        "        convRelu(0)\n",
        "        cnn.add_module('pooling{0}'.format(0), nn.MaxPool2d(2, 2))  # 64x16x64\n",
        "        convRelu(1)\n",
        "        cnn.add_module('pooling{0}'.format(1), nn.MaxPool2d(2, 2))  # 128x8x32\n",
        "        convRelu(2, True)\n",
        "        convRelu(3)\n",
        "        cnn.add_module('pooling{0}'.format(2),\n",
        "                       nn.MaxPool2d((2, 2), (2, 1), (0, 1)))  # 256x4x16\n",
        "        convRelu(4, True)\n",
        "        convRelu(5)\n",
        "        cnn.add_module('pooling{0}'.format(3),\n",
        "                       nn.MaxPool2d((2, 2), (2, 1), (0, 1)))  # 512x2x16\n",
        "        convRelu(6, True)  # 512x1x16\n",
        "        self.cnn = cnn\n",
        "        self.rnn = nn.Sequential()\n",
        "        self.rnn = nn.Sequential(\n",
        "            BidirectionalLSTM(opt['nHidden']*2, opt['nHidden'], opt['nHidden']),\n",
        "            BidirectionalLSTM(opt['nHidden'], opt['nHidden'], opt['nClasses']))\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        # conv features\n",
        "        conv = self.cnn(input)\n",
        "        b, c, h, w = conv.size()\n",
        "        assert h == 1, \"the height of conv must be 1\"\n",
        "        conv = conv.squeeze(2)\n",
        "        conv = conv.permute(2, 0, 1)  # [w, b, c]\n",
        "        # rnn features\n",
        "        output = self.rnn(conv)\n",
        "        output = output.transpose(1,0) #Tbh to bth\n",
        "        return output"
      ],
      "metadata": {
        "id": "o_v0Oi5Itd7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#The CTC Loss\n",
        "\n",
        "CTC was proposed by [A. Graves et al.].\n",
        "We don't need exasct alignment between input and output.\n",
        "The CTC layer computes a score with all possible alignments of the target label.\n",
        "The OCR is trained to maximize the sum of these scores."
      ],
      "metadata": {
        "id": "NCs5RRMG1QG8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title The CTC Loss\n",
        "\n",
        "# Code by Jerin Philip\n",
        "# https://jerinphilip.github.io/\n",
        "\n",
        "class CustomCTCLoss(torch.nn.Module):\n",
        "    # T x B x H => Softmax on dimension 2\n",
        "    def __init__(self, dim=2):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.ctc_loss = torch.nn.CTCLoss(reduction='mean', zero_infinity=True)\n",
        "\n",
        "    def forward(self, logits, labels,\n",
        "            prediction_sizes, target_sizes):\n",
        "        EPS = 1e-7\n",
        "        loss = self.ctc_loss(logits, labels, prediction_sizes, target_sizes)\n",
        "        loss = self.sanitize(loss)\n",
        "        return self.debug(loss, logits, labels, prediction_sizes, target_sizes)\n",
        "    \n",
        "    def sanitize(self, loss):\n",
        "        EPS = 1e-7\n",
        "        if abs(loss.item() - float('inf')) < EPS:\n",
        "            return torch.zeros_like(loss)\n",
        "        if math.isnan(loss.item()):\n",
        "            return torch.zeros_like(loss)\n",
        "        return loss\n",
        "\n",
        "    def debug(self, loss, logits, labels,\n",
        "            prediction_sizes, target_sizes):\n",
        "        if math.isnan(loss.item()):\n",
        "            print(\"Loss:\", loss)\n",
        "            print(\"logits:\", logits)\n",
        "            print(\"labels:\", labels)\n",
        "            print(\"prediction_sizes:\", prediction_sizes)\n",
        "            print(\"target_sizes:\", target_sizes)\n",
        "            raise Exception(\"NaN loss obtained. But why?\")\n",
        "        return loss"
      ],
      "metadata": {
        "id": "dCSq8JZ71ESr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Training loop\n",
        "\n",
        "class OCRTrainer(object):\n",
        "    def __init__(self, opt):\n",
        "        super(OCRTrainer, self).__init__()\n",
        "        self.data_train = opt['data_train']\n",
        "        self.data_val = opt['data_val']\n",
        "        self.model = opt['model']\n",
        "        self.criterion = opt['criterion']\n",
        "        self.optimizer = opt['optimizer']\n",
        "        self.schedule = opt['schedule']\n",
        "        self.converter = OCRLabelConverter(opt['alphabet'])\n",
        "        self.evaluator = Eval()\n",
        "        print('Scheduling is {}'.format(self.schedule))\n",
        "        self.scheduler = CosineAnnealingLR(self.optimizer, T_max=opt['epochs'])\n",
        "        self.batch_size = opt['batch_size']\n",
        "        self.count = opt['epoch']\n",
        "        self.epochs = opt['epochs']\n",
        "        self.cuda = opt['cuda']\n",
        "        self.collate_fn = opt['collate_fn']\n",
        "        self.init_meters()\n",
        "\n",
        "    def init_meters(self):\n",
        "        self.avgTrainLoss = AverageMeter(\"Train loss\")\n",
        "        self.avgTrainCharAccuracy = AverageMeter(\"Train Character Accuracy\")\n",
        "        self.avgTrainWordAccuracy = AverageMeter(\"Train Word Accuracy\")\n",
        "        self.avgValLoss = AverageMeter(\"Validation loss\")\n",
        "        self.avgValCharAccuracy = AverageMeter(\"Validation Character Accuracy\")\n",
        "        self.avgValWordAccuracy = AverageMeter(\"Validation Word Accuracy\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.model(x)\n",
        "        return logits.transpose(1, 0)\n",
        "\n",
        "    def loss_fn(self, logits, targets, pred_sizes, target_sizes):\n",
        "        loss = self.criterion(logits, targets, pred_sizes, target_sizes)\n",
        "        return loss\n",
        "\n",
        "    def step(self):\n",
        "        self.max_grad_norm = 0.05\n",
        "        clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n",
        "        self.optimizer.step()\n",
        "    \n",
        "    def schedule_lr(self):\n",
        "        if self.schedule:\n",
        "            self.scheduler.step()\n",
        "\n",
        "    def _run_batch(self, batch, report_accuracy=False, validation=False):\n",
        "        input_, targets = batch['img'], batch['label']\n",
        "        targets, lengths = self.converter.encode(targets)\n",
        "        logits = self.forward(input_)\n",
        "        logits = logits.contiguous().cpu()\n",
        "        logits = torch.nn.functional.log_softmax(logits, 2)\n",
        "        T, B, H = logits.size()\n",
        "        pred_sizes = torch.LongTensor([T for i in range(B)])\n",
        "        targets= targets.view(-1).contiguous()\n",
        "        loss = self.loss_fn(logits, targets, pred_sizes, lengths)\n",
        "        if report_accuracy:\n",
        "            probs, preds = logits.max(2)\n",
        "            preds = preds.transpose(1, 0).contiguous().view(-1)\n",
        "            sim_preds = self.converter.decode(preds.data, pred_sizes.data, raw=False)\n",
        "            ca = np.mean((list(map(self.evaluator.char_accuracy, list(zip(sim_preds, batch['label']))))))\n",
        "            wa = np.mean((list(map(self.evaluator.word_accuracy, list(zip(sim_preds, batch['label']))))))\n",
        "        return loss, ca, wa\n",
        "\n",
        "    def run_epoch(self, validation=False):\n",
        "        if not validation:\n",
        "            loader = self.train_dataloader()\n",
        "            pbar = tqdm(loader, desc='Epoch: [%d]/[%d] Training'%(self.count, \n",
        "                self.epochs), leave=True)\n",
        "            self.model.train()\n",
        "        else:\n",
        "            loader = self.val_dataloader()\n",
        "            pbar = tqdm(loader, desc='Validating', leave=True)\n",
        "            self.model.eval()\n",
        "        outputs = []\n",
        "        for batch_nb, batch in enumerate(pbar):\n",
        "            if not validation:\n",
        "                output = self.training_step(batch)\n",
        "            else:\n",
        "                output = self.validation_step(batch)\n",
        "            pbar.set_postfix(output)\n",
        "            outputs.append(output)\n",
        "        self.schedule_lr()\n",
        "        if not validation:\n",
        "            result = self.train_end(outputs)\n",
        "        else:\n",
        "            result = self.validation_end(outputs)\n",
        "        return result\n",
        "\n",
        "    def training_step(self, batch):\n",
        "        loss, ca, wa = self._run_batch(batch, report_accuracy=True)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.step()\n",
        "        output = OrderedDict({\n",
        "            'loss': abs(loss.item()),\n",
        "            'train_ca': ca.item(),\n",
        "            'train_wa': wa.item()\n",
        "            })\n",
        "        return output\n",
        "\n",
        "    def validation_step(self, batch):\n",
        "        loss, ca, wa = self._run_batch(batch, report_accuracy=True, validation=True)\n",
        "        output = OrderedDict({\n",
        "            'val_loss': abs(loss.item()),\n",
        "            'val_ca': ca.item(),\n",
        "            'val_wa': wa.item()\n",
        "            })\n",
        "        return output\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        # logging.info('training data loader called')\n",
        "        loader = torch.utils.data.DataLoader(self.data_train,\n",
        "                batch_size=self.batch_size,\n",
        "                collate_fn=self.collate_fn,\n",
        "                shuffle=True)\n",
        "        return loader\n",
        "        \n",
        "    def val_dataloader(self):\n",
        "        # logging.info('val data loader called')\n",
        "        loader = torch.utils.data.DataLoader(self.data_val,\n",
        "                batch_size=self.batch_size,\n",
        "                collate_fn=self.collate_fn)\n",
        "        return loader\n",
        "\n",
        "    def train_end(self, outputs):\n",
        "        for output in outputs:\n",
        "            self.avgTrainLoss.add(output['loss'])\n",
        "            self.avgTrainCharAccuracy.add(output['train_ca'])\n",
        "            self.avgTrainWordAccuracy.add(output['train_wa'])\n",
        "\n",
        "        train_loss_mean = abs(self.avgTrainLoss.compute())\n",
        "        train_ca_mean = self.avgTrainCharAccuracy.compute()\n",
        "        train_wa_mean = self.avgTrainWordAccuracy.compute()\n",
        "\n",
        "        result = {'train_loss': train_loss_mean, 'train_ca': train_ca_mean,\n",
        "        'train_wa': train_wa_mean}\n",
        "        # result = {'progress_bar': tqdm_dict, 'log': tqdm_dict, 'val_loss': train_loss_mean}\n",
        "        return result\n",
        "\n",
        "    def validation_end(self, outputs):\n",
        "        for output in outputs:\n",
        "            self.avgValLoss.add(output['val_loss'])\n",
        "            self.avgValCharAccuracy.add(output['val_ca'])\n",
        "            self.avgValWordAccuracy.add(output['val_wa'])\n",
        "\n",
        "        val_loss_mean = abs(self.avgValLoss.compute())\n",
        "        val_ca_mean = self.avgValCharAccuracy.compute()\n",
        "        val_wa_mean = self.avgValWordAccuracy.compute()\n",
        "\n",
        "        result = {'val_loss': val_loss_mean, 'val_ca': val_ca_mean,\n",
        "        'val_wa': val_wa_mean}\n",
        "        return result"
      ],
      "metadata": {
        "id": "sJZYX3YF5JuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Putting everything together\n",
        "\n",
        "# Source code from https://deepayan137.github.io/blog/markdown/2020/08/29/building-ocr.html\n",
        "# Under MIT license\n",
        "\n",
        "class Learner(object):\n",
        "    def __init__(self, model, optimizer, savepath=None, resume=False):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.savepath = os.path.join(savepath, 'best.ckpt')\n",
        "        self.cuda = torch.cuda.is_available() \n",
        "        self.cuda_count = torch.cuda.device_count()\n",
        "        if self.cuda:\n",
        "            self.model = self.model.cuda()\n",
        "        self.epoch = 0\n",
        "        if self.cuda_count > 1:\n",
        "            print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "            self.model = nn.DataParallel(self.model)\n",
        "        self.best_score = None\n",
        "        if resume and os.path.exists(self.savepath):\n",
        "            self.checkpoint = torch.load(self.savepath)\n",
        "            self.epoch = self.checkpoint['epoch']\n",
        "            self.best_score=self.checkpoint['best']\n",
        "            self.load()\n",
        "        else:\n",
        "            print('checkpoint does not exist')\n",
        "\n",
        "    def fit(self, opt):\n",
        "        opt['cuda'] = self.cuda\n",
        "        opt['model'] = self.model\n",
        "        opt['optimizer'] = self.optimizer\n",
        "        logging.basicConfig(filename=\"%s/%s.csv\" %(opt['log_dir'], opt['name']), level=logging.INFO)\n",
        "        self.saver = EarlyStopping(self.savepath, patience=15, verbose=True, best_score=self.best_score)\n",
        "        opt['epoch'] = self.epoch\n",
        "        trainer = OCRTrainer(opt)\n",
        "        \n",
        "        for epoch in range(opt['epoch'], opt['epochs']):\n",
        "            train_result = trainer.run_epoch()\n",
        "            val_result = trainer.run_epoch(validation=True)\n",
        "            trainer.count = epoch\n",
        "            info = '%d, %.6f, %.6f, %.6f, %.6f, %.6f, %.6f'%(epoch, train_result['train_loss'], \n",
        "                val_result['val_loss'], train_result['train_ca'],  val_result['val_ca'],\n",
        "                train_result['train_wa'], val_result['val_wa'])\n",
        "            logging.info(info)\n",
        "            self.val_loss = val_result['val_loss']\n",
        "            print(self.val_loss)\n",
        "            if self.savepath:\n",
        "                self.save(epoch)\n",
        "            if self.saver.early_stop:\n",
        "                print(\"Early stopping\")\n",
        "                break\n",
        "\n",
        "    def load(self):\n",
        "        print('Loading checkpoint at {} trained for {} epochs'.format(self.savepath, self.checkpoint['epoch']))\n",
        "        self.model.load_state_dict(self.checkpoint['state_dict'])\n",
        "        if 'opt_state_dict' in self.checkpoint.keys():\n",
        "            print('Loading optimizer')\n",
        "            self.optimizer.load_state_dict(self.checkpoint['opt_state_dict'])\n",
        "\n",
        "    def save(self, epoch):\n",
        "        self.saver(self.val_loss, epoch, self.model, self.optimizer)"
      ],
      "metadata": {
        "id": "okmArj4xAa2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Learning\n",
        "\n",
        "# Source code from https://deepayan137.github.io/blog/markdown/2020/08/29/building-ocr.html\n",
        "# MIT license\n",
        "\n",
        "alphabet = \"\"\"Only thewigsofrcvdampbkuq.$A-210xT5'MDL,RYHJ\"ISPWENj&BC93VGFKz();#:!7U64Q8?+*ZX/%\"\"\"\n",
        "args = {\n",
        "    'name':'exp1',\n",
        "    'path':'data',\n",
        "    'imgdir': 'train',\n",
        "    'imgH':32,\n",
        "    'nChannels':1,\n",
        "    'nHidden':256,\n",
        "    'nClasses':len(alphabet),\n",
        "    'lr':0.001,\n",
        "    'epochs':4,\n",
        "    'batch_size':32,\n",
        "    'save_dir':'checkpoints',\n",
        "    'log_dir':'logs',\n",
        "    'resume':False,\n",
        "    'cuda':False,\n",
        "    'schedule':False\n",
        "    \n",
        "}\n",
        "\n",
        "data = SynthDataset(args)\n",
        "args['collate_fn'] = SynthCollator()\n",
        "train_split = int(0.8*len(data))\n",
        "val_split = len(data) - train_split\n",
        "args['data_train'], args['data_val'] = random_split(data, (train_split, val_split))\n",
        "print('Traininig Data Size:{}\\nVal Data Size:{}'.format(\n",
        "    len(args['data_train']), len(args['data_val'])))\n",
        "args['alphabet'] = alphabet\n",
        "model = CRNN(args)\n",
        "args['criterion'] = CustomCTCLoss()\n",
        "savepath = os.path.join(args['save_dir'], args['name'])\n",
        "gmkdir(savepath)\n",
        "gmkdir(args['log_dir'])\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
        "learner = Learner(model, optimizer, savepath=savepath, resume=args['resume'])\n",
        "learner.fit(args)"
      ],
      "metadata": {
        "id": "KUZsGlXhBX1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation and testing\n",
        "\n"
      ],
      "metadata": {
        "id": "Vnjn8zG-CUQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "def get_accuracy(args):\n",
        "    loader = torch.utils.data.DataLoader(args['data'],\n",
        "                batch_size=args['batch_size'],\n",
        "                collate_fn=args['collate_fn'])\n",
        "    model = args['model']\n",
        "    model.eval()\n",
        "    converter = OCRLabelConverter(args['alphabet'])\n",
        "    evaluator = Eval()\n",
        "    labels, predictions, images = [], [], []\n",
        "    for iteration, batch in enumerate(tqdm(loader)):\n",
        "        input_, targets = batch['img'].to(device), batch['label']\n",
        "        images.extend(input_.squeeze().detach())\n",
        "        labels.extend(targets)\n",
        "        targets, lengths = converter.encode(targets)\n",
        "        logits = model(input_).transpose(1, 0)\n",
        "        logits = torch.nn.functional.log_softmax(logits, 2)\n",
        "        logits = logits.contiguous().cpu()\n",
        "        T, B, H = logits.size()\n",
        "        pred_sizes = torch.LongTensor([T for i in range(B)])\n",
        "        probs, pos = logits.max(2)\n",
        "        pos = pos.transpose(1, 0).contiguous().view(-1)\n",
        "        sim_preds = converter.decode(pos.data, pred_sizes.data, raw=False)\n",
        "        predictions.extend(sim_preds)\n",
        "        \n",
        "#     make_grid(images[:10], nrow=2)\n",
        "    fig=plt.figure(figsize=(8, 8))\n",
        "    columns = 4\n",
        "    rows = 5\n",
        "    for i in range(1, columns*rows +1):\n",
        "        img = images[i]\n",
        "        img = (img - img.min())/(img.max() - img.min())\n",
        "        img = np.array(img * 255.0, dtype=np.uint8)\n",
        "        fig.add_subplot(rows, columns, i)\n",
        "        plt.title(predictions[i])\n",
        "        plt.axis('off')\n",
        "        plt.imshow(img)\n",
        "    plt.show()\n",
        "    ca = np.mean((list(map(evaluator.char_accuracy, list(zip(predictions, labels))))))\n",
        "    wa = np.mean((list(map(evaluator.word_accuracy_line, list(zip(predictions, labels))))))\n",
        "    return ca, wa\n"
      ],
      "metadata": {
        "id": "WFEU24AqChrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args['imgdir'] = 'test'\n",
        "args['data'] = SynthDataset(args)\n",
        "resume_file = os.path.join(args['save_dir'], args['name'], 'best.ckpt')\n",
        "if os.path.isfile(resume_file):\n",
        "    print('Loading model %s'%resume_file)\n",
        "    checkpoint = torch.load(resume_file)\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    args['model'] = model\n",
        "    ca, wa = get_accuracy(args)\n",
        "    print(\"Character Accuracy: %.2f\\nWord Accuracy: %.2f\"%(ca, wa))\n",
        "else:\n",
        "    print(\"=> no checkpoint found at '{}'\".format(save_file))\n",
        "    print('Exiting')"
      ],
      "metadata": {
        "id": "r2NqixpbCl5b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
