\documentclass{article}
\usepackage[utf8]{inputenc}

\title{OCR}

\begin{document}

\maketitle

\section{Introduction}


We are concerned with a classic problem in computer vision, image-based sequence recognition. Recognition of sequence-like objects, such as scene text, handwriting and musical score, tend to take place in the form of a series of labels, rather than a single label. RNNs can be trained and optimized in an end-to-end fashion, but require complex post-processing steps before they are ready for use. The CRNN is a combination of the DCNN and RNN, and constructs an end-to-end system for sequence recogniations. It can be directly learned from sequence labels, requiring no detailed annotations (for instance, words) and requires only height normalization in training and testing phases.\\

CRNN is a type of recurrent neural network (RNN) that can be trained to make pre-diction for each frame of a feature sequence, outputted by the convolutional layers. In CRNN, each column of the feature maps corresponds to a.rectangle region of the original image (termed the receptive field), and regions are in the same order as their corresponding columns on the feature maps from left to right. A deep bidirectional Recurrent Neural Network (CRNN) is built on the top of convolutional layers, as the recurrent layers.
The recurrent layers predict a label distribution yt for each frame xt in the feature sequence. Each time it receives a frame in the sequence, it updates its internal state with a non-linear function that takes both current input and past state as its input. Long-Short-Term Memory (LSTM) is a type of RNN unit that is designed to capture long-range dependencies, which often occur in image-based sequences. LSTM consists of a memory cell and three multiplicative gates, namely the input, output and forget gates. It has achieved significant performance improvements in the task of speech recognition.\\

Transcription is the process of converting the per-frame predictions made by RNN into a label sequence. We adopt the conditional probability defined in the Connectionist Temporal Classification (CTC) proposed by Graves et al. In CTC, each input is a sequence y = y1, ... , yT where T is the sequence length.
We have trained a neural network that can be end-to-end trained on pairs of images, eliminating the procedure of manually labeling all individual components in training images. The network is trained with stochastic gradient descent (SGD) and error differentials calculated by the back-propagation algorithm.\\

we describe the training and testing of the proposed CRNN model for scene text recognition and musical score recognition. We use a dataset of 8 million training images and their corresponding ground truth words as well as a 50k word lexicon to train the model. We train our model on rescaled training images, whose sizes are 100 * 32, for about 250k iterations. Then, we train with variable-size images for another 50k iterations before adding batch normalization layers.\\

CRNN is not limited to recognizing a word in a known dictionary, and able to handle andom strings (e.g., telephone numbers), sentences or other scripts like Chinese words. We obtain superior performance on IIIT5k, and SVT compared to  and only achieved lower performance on IC03 with the "Full" lexicon. In CRNN, all layers have weight-sharing connecÂ­nections, and the fully-connected layers are not needed. For example, removing the 4th and 6th convolutional layers ends up with 86.5 percent accuracy on the validation set of test images.\\

We plot the recognition accuracy as a function of d. Larger d results in more candidates,thus more accurate lexicon-based transcription. On the other hand, the computational cost grows with larger d, due to the longer BKtree search time. CRNN outperforms commercial OMR engines, such as the Capella Scan and PhotoScore, by a large margin.\\

The CRNN uses convolutional features that are highly robust to noise and distortion. It can be easily applied to other image-based sequence recognition probabilites, requiring minimal domain knowledge.
We have presented CRNN, a novel neural network architecture that integrates the advantages of both Deep Convolutional Neural and Recurrent Neural Networks. It directly runs on coarse level labels (e.g., words) and requires no detailed annotations for each individual eleventh (i.e., characters) in the training phase.




\end{document}
